{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "oDJDtJzaEyll",
    "outputId": "78a5b89f-931d-4998-cb5e-6091c4032c8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R_fs6UcSFBsJ"
   },
   "outputs": [],
   "source": [
    "!unzip -q \"./drive/My Drive/Colab Notebooks/DL_Project/Dataset.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QtlrPo5zFVof"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()\n",
    "path = 'Dataset/'\n",
    "train_image_path = path + 'images/images_train/'\n",
    "validation_image_path = path + 'images/images_validation/'\n",
    "test_image_path = path + 'images/images_test/'\n",
    "train_formula_path = path + 'formulas/train_formulas.txt'\n",
    "validation_formula_path = path + 'formulas/validation_formulas.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PdNx2aFMF7XR"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_train, n_validation, n_test = 66509, 7391, 8250\n",
    "\n",
    "images_train = []\n",
    "images_validation = []\n",
    "images_test = []\n",
    "\n",
    "for i in range(n_train):\n",
    "  img = np.asarray(Image.open(train_image_path + '{}.png'.format(i)))\n",
    "  images_train.append(img)\n",
    "\n",
    "for i in range(n_validation):\n",
    "  img = np.asarray(Image.open(validation_image_path + '{}.png'.format(i)))\n",
    "  images_validation.append(img)\n",
    "\n",
    "for i in range(n_test):\n",
    "  img = np.asarray(Image.open(test_image_path + '{}.png'.format(i)))\n",
    "  images_test.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6yRWXhZYTau"
   },
   "outputs": [],
   "source": [
    "images_train = np.array(images_train)\n",
    "images_validation = np.array(images_validation)\n",
    "images_test = np.array(images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6vuJapYBZLUr"
   },
   "outputs": [],
   "source": [
    "images_train = images_train.reshape(n_train, 1, 60, 400)\n",
    "images_validation = images_validation.reshape(n_validation, 1, 60, 400)\n",
    "images_test = images_test.reshape(n_test, 1, 60, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gCcLZ8yHKcfe"
   },
   "outputs": [],
   "source": [
    "train_formulas = open(train_formula_path, 'r').readlines()\n",
    "validation_formulas = open(validation_formula_path, 'r').readlines()\n",
    "bos_token = '<BOS>'\n",
    "eos_token = '<EOS>'\n",
    "pad_token = '<PAD>'\n",
    "unk_token = '<UNK>'\n",
    "\n",
    "MAX_LENGTH = 0\n",
    "\n",
    "for i in range(n_train):\n",
    "  train_formulas[i] = [bos_token] + train_formulas[i].split() + [eos_token]\n",
    "  MAX_LENGTH = max(MAX_LENGTH, len(train_formulas[i]))\n",
    "\n",
    "for i in range(n_validation):\n",
    "  validation_formulas[i] = [bos_token] + validation_formulas[i].split() + [eos_token]\n",
    "  MAX_LENGTH = max(MAX_LENGTH, len(validation_formulas[i]))\n",
    "\n",
    "train_formula_lengths = np.zeros((n_train,))\n",
    "validation_formula_lengths = np.zeros((n_validation,))\n",
    "# print(MAX_LENGTH)\n",
    "\n",
    "for i in range(n_train):\n",
    "  length = len(train_formulas[i])\n",
    "  train_formula_lengths[i] = length\n",
    "  train_formulas[i] += [pad_token for _ in range(MAX_LENGTH - length)]\n",
    "\n",
    "for i in range(n_validation):\n",
    "  length = len(validation_formulas[i])\n",
    "  validation_formula_lengths[i] = length\n",
    "  validation_formulas[i] += [pad_token for _ in range(MAX_LENGTH - length)]\n",
    "\n",
    "\n",
    "# print(len(train_formulas))\n",
    "# print(len(validation_formulas))\n",
    "\n",
    "# print(train_formulas[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xZi-CTr_WuKN"
   },
   "outputs": [],
   "source": [
    "train_formula_lengths = train_formula_lengths.astype(int)\n",
    "validation_formula_lengths = validation_formula_lengths.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "od05BKYVKxPQ"
   },
   "outputs": [],
   "source": [
    "all_tokens = [unk_token]\n",
    "for i in range(n_train):\n",
    "  for j in range(len(train_formulas[i])):\n",
    "    if train_formulas[i][j] not in all_tokens:\n",
    "      all_tokens.append(train_formulas[i][j])\n",
    "#   all_tokens += train_formulas[i]\n",
    "\n",
    "# all_tokens = list(set(all_tokens))\n",
    "num_tokens = len(all_tokens)\n",
    "# print(num_tokens)\n",
    "# print(all_tokens)\n",
    "# print('\\\\over' in all_tokens)\n",
    "# print('\\\\' in all_tokens)\n",
    "token_idx = {}\n",
    "for i in range(num_tokens):\n",
    "  token_idx[all_tokens[i]] = i\n",
    "# print(token_idx['<BOS>'])\n",
    "# print(token_idx[bos_token])\n",
    "# print(token_idx['{'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "IPNdzUoPj2jy",
    "outputId": "0f5e855e-b689-4161-b9d1-686731954e13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\bigg\n",
      "34\n",
      "566\n"
     ]
    }
   ],
   "source": [
    "print(all_tokens[34])\n",
    "print(token_idx['\\\\bigg'])\n",
    "print(len(all_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UnFzPgvdL_uk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V4qHiIXBtXks"
   },
   "outputs": [],
   "source": [
    "for i in range(n_train):\n",
    "  for j in range(MAX_LENGTH):\n",
    "    train_formulas[i][j] = token_idx[train_formulas[i][j]]\n",
    "\n",
    "for i in range(n_validation):\n",
    "  for j in range(MAX_LENGTH):\n",
    "    validation_formulas[i][j] = token_idx.get(validation_formulas[i][j], token_idx[unk_token])\n",
    "\n",
    "train_formulas = torch.LongTensor(train_formulas)\n",
    "validation_formulas = torch.LongTensor(validation_formulas)\n",
    "\n",
    "# print(train_formulas.shape)\n",
    "# print(validation_formulas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wVp_80M-L__A"
   },
   "outputs": [],
   "source": [
    "CUDA = True\n",
    "BATCH_SIZE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cFukL1a9MAFw"
   },
   "outputs": [],
   "source": [
    "class MyReshape(nn.Module):\n",
    "  def forward(self, input):\n",
    "    return input.view(input.size(0), input.size(1)*input.size(2), input.size(3)).transpose(1,2)\n",
    "\n",
    "\n",
    "cnn_encoder = nn.Sequential(\n",
    "  nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "  nn.ReLU(),\n",
    "  \n",
    "  nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0)),\n",
    "  \n",
    "  nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "  nn.ReLU(),\n",
    "  \n",
    "  nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0)),\n",
    "  \n",
    "  nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "  nn.BatchNorm2d(256),\n",
    "  nn.ReLU(),\n",
    "  \n",
    "  nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "  nn.ReLU(),\n",
    "  \n",
    "  nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=(0, 0)),\n",
    "  \n",
    "  nn.Conv2d(in_channels=256, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "  nn.BatchNorm2d(512),\n",
    "  nn.ReLU(),\n",
    "  \n",
    "  nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=(0, 0)),\n",
    "  \n",
    "  nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "  nn.BatchNorm2d(512),\n",
    "  nn.ReLU(),\n",
    "  \n",
    "  MyReshape()\n",
    "    \n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "01aO_LLhOI0H",
    "outputId": "e0ccd85d-737d-46a3-b953-a4750848cdbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 50, 3584])\n"
     ]
    }
   ],
   "source": [
    "dummy_v = cnn_encoder(torch.Tensor(images_validation[0:10]).cuda())\n",
    "print(dummy_v.shape)\n",
    "_, seq_len, feature_num = dummy_v.shape\n",
    "encoder_hidden_size = 256\n",
    "num_layers = 1\n",
    "\n",
    "lstm_encoder = nn.LSTM(feature_num, encoder_hidden_size, num_layers, bidirectional=True, batch_first=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t3ofh2XMopex"
   },
   "outputs": [],
   "source": [
    "v = cnn_encoder(torch.Tensor(images_validation[0:10]).cuda())\n",
    "# print(v.shape)\n",
    "# print(lstm_encoder.device)\n",
    "v_tilda, (hn, cn) = lstm_encoder(v)\n",
    "# print(v_tilda.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "blXIDyCrcIqK"
   },
   "outputs": [],
   "source": [
    "class AttnDecoder(nn.Module):\n",
    "  def __init__(self, num_tokens, embedding_dim, hidden_dim, output_dim, attn_dim):\n",
    "    super(AttnDecoder, self).__init__()\n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.output_dim = output_dim\n",
    "    self.attn_dim = attn_dim # dimension e Wh o Wv o Beta\n",
    "    \n",
    "    self.embedding = nn.Embedding(num_tokens, embedding_dim)\n",
    "    self.lstm = nn.LSTM(embedding_dim+output_dim, hidden_dim, 1, batch_first=True)\n",
    "    self.Wh = nn.Linear(hidden_dim, attn_dim)\n",
    "    self.Wv = nn.Linear(hidden_dim, attn_dim)\n",
    "    self.tanh = nn.Tanh()\n",
    "    self.beta = nn.Linear(attn_dim,1)\n",
    "    self.Wc1 = nn.Linear(hidden_dim, output_dim)\n",
    "    self.Wc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    self.o_tanh = nn.Tanh()\n",
    "    self.Wout = nn.Linear(output_dim, num_tokens)\n",
    "    self.nll = nn.NLLLoss(reduction='sum')\n",
    "    \n",
    "    \n",
    "  def forward(self, last_y, last_o, last_h, last_c_memory, v, ):\n",
    "    \n",
    "    linear_trans = self.Wh(last_h).unsqueeze(1) + self.Wv(v) # Wh * h_t-1 + Wv * v_t\n",
    "    e_i = self.beta(self.tanh(linear_trans)).squeeze(2) # beta_T * tanh(linear_trans)\n",
    "    alpha_i = F.softmax(e_i, dim=1)\n",
    "    c_i = (alpha_i.unsqueeze(2) * v).sum(dim=1)\n",
    "    \n",
    "    embedded = self.embedding(last_y)\n",
    "    ltsm_input = torch.cat((embedded, last_o), dim=1)\n",
    "    ltsm_input = ltsm_input.unsqueeze(1)\n",
    "    h_t, (_, c_memory_t) = self.lstm(ltsm_input, (last_h.unsqueeze(0), last_c_memory.unsqueeze(0)))\n",
    "    h_t = h_t.squeeze(1)\n",
    "    c_memory_t = c_memory_t.squeeze(0)\n",
    "    \n",
    "    o_t = self.o_tanh(self.Wc1(h_t) + self.Wc2(c_i))\n",
    "    prob = F.log_softmax(self.Wout(o_t), dim=1)\n",
    "    \n",
    "    return prob, o_t, h_t, c_memory_t,\n",
    "    \n",
    "  def train(self, v_tilda, sequence, max_len, teacher_forcing_ratio=1):\n",
    "    batch_num = v_tilda.shape[0]\n",
    "    h_0 = v_tilda[:,-1,:].contiguous()\n",
    "    c_0 = torch.zeros_like(h_0).cuda()\n",
    "    o_0 = torch.zeros(batch_num, self.output_dim).cuda()\n",
    "    y_0 = sequence[:, 0].cuda()\n",
    "    \n",
    "    loss = 0\n",
    "    for i in range(max_len-1):\n",
    "#       y_0 = sequence[:, i].cuda()\n",
    "      prob, o_0, h_0, c_0 = self.forward(y_0, o_0, h_0, c_0, v_tilda)\n",
    "      loss += self.nll(prob, sequence[:, i+1].cuda())\n",
    "      \n",
    "      y_0 = sequence[:, i+1]\n",
    "#       eps = random.random()\n",
    "#       if eps > teacher_forcing_ratio:\n",
    "#         y_0 = torch.distributions.Categorical(prob).sample()\n",
    "#       else:\n",
    "#         y_0 = sequence[:, i+1]\n",
    "      \n",
    "    return loss\n",
    "  \n",
    "      \n",
    "\n",
    "attn_decoder = AttnDecoder(num_tokens, embedding_dim=80, hidden_dim=512, output_dim=600, attn_dim=512).cuda()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ViQROauEymoK"
   },
   "outputs": [],
   "source": [
    "def predict(image, cnn_encoder, lstm_encoder, attn_decoder, max_len=178):\n",
    "  v = cnn_encoder(image.unsqueeze(0))\n",
    "  v_tilda, _ = lstm_encoder(v)\n",
    "  \n",
    "  h_0 = v_tilda[0, -1, :].unsqueeze(0)\n",
    "  c_0 = torch.zeros_like(h_0).cuda()\n",
    "  o_0 = torch.zeros(attn_decoder.output_dim).unsqueeze(0).cuda()\n",
    "  \n",
    "  sequence = [token_idx[bos_token]]\n",
    "  \n",
    "  for i in range(max_len-1):\n",
    "    y_0 = torch.LongTensor([sequence[-1]]).cuda()\n",
    "    prob, o_0, h_0, c_0 = attn_decoder(y_0, o_0, h_0, c_0, v_tilda)\n",
    "    prob = prob.squeeze(0)\n",
    "    max_idx = torch.argmax(prob).item()\n",
    "#     print(2.71 ** prob[max_idx].item())\n",
    "    sequence.append(max_idx)\n",
    "    if max_idx == token_idx[eos_token] or max_idx == token_idx[pad_token]:\n",
    "      return sequence\n",
    "    \n",
    "  return sequence\n",
    "      \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-Fh9iDY0dfLw",
    "outputId": "8a03a824-9355-4c28-bd64-e45e4626480d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D _ { \\mu } = \\partial _ { \\mu } + A _ { \\mu } , \\qquad A _ { \\mu } = i g A ^ { a } _ { \\mu } ( x ) T ^ { a } \n"
     ]
    }
   ],
   "source": [
    "index = 1\n",
    "test_image = torch.Tensor(images_validation[index]).cuda()\n",
    "test_image -= 128\n",
    "test_image /= 128\n",
    "formula = predict(test_image, cnn_encoder, lstm_encoder, attn_decoder)\n",
    "for i in range(len(formula)):\n",
    "  formula[i] = all_tokens[formula[i]]\n",
    "# print(formula)\n",
    "sentence = \"\"\n",
    "for i in range(1, len(formula)-1):\n",
    "  sentence += formula[i]\n",
    "  sentence += \" \"\n",
    "print(sentence)\n",
    "# formula = ''.join(formula[1:-1], )\n",
    "# print(formula)\n",
    "\n",
    "# plt.imshow(images_validation[index].reshape(60,400))\n",
    "# plt.show()\n",
    "# print(images_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "IXmku5AoocR6",
    "outputId": "517a9015-36b4-41a0-e1e3-bdaf16ea553f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13784.0469, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "last_y = torch.LongTensor([0,1,2,3,4,5,6,7,8,9])\n",
    "# print(attn_decoder.embedding(last_y))\n",
    "# kkk = attn_decoder.embedding(last_y)\n",
    "# print(kkk.shape)\n",
    "# print(torch.cat((kkk, torch.randn(10, 564)), dim=1).shape)\n",
    "attn_decoder(last_y.cuda(), torch.randn(10, 600).cuda(), torch.randn(10,512).cuda(), torch.randn(10,512).cuda(), torch.randn(10,50,512).cuda())\n",
    "# print(type(train_formulas))\n",
    "print(attn_decoder.train(torch.randn(20,50,512).cuda(), train_formulas[0:20].cuda(), max(train_formula_lengths[0:20])))\n",
    "# print(attn_decoder.train(torch.randn(20,50,512), train_formulas[0:20])[0].shape)\n",
    "\n",
    "# attn_decoder.test(torch.randn(20, 50, 512).cuda())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UgWlbUUc8vCq"
   },
   "outputs": [],
   "source": [
    "def make_batches():\n",
    "  counter = 0\n",
    "  while True:\n",
    "    start = counter * BATCH_SIZE\n",
    "    end = min((counter+1) * BATCH_SIZE, n_train)\n",
    "    if end == n_train:\n",
    "      counter = 0\n",
    "    x = torch.Tensor(images_train[start:end]).cuda()\n",
    "    y = train_formulas[start:end].cuda()\n",
    "    max_len = max(train_formula_lengths[start:end])\n",
    "    counter += 1\n",
    "    x -= 128\n",
    "    x /= 128\n",
    "    \n",
    "    yield x, y, max_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "--5_NJwE7eOy"
   },
   "outputs": [],
   "source": [
    "batch_maker = make_batches()\n",
    "# print(train_formulas[0])\n",
    "print(next(batch_maker)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mrHMnD0axfPj"
   },
   "outputs": [],
   "source": [
    "attn_decoder_params = list(attn_decoder.parameters())\n",
    "print(attn_decoder.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L1Xnq9ef8i4w"
   },
   "outputs": [],
   "source": [
    "teacher_forcing = 1\n",
    "N_EPOCH = 12\n",
    "lr = 0.001\n",
    "\n",
    "params = list(cnn_encoder.parameters()) + list(lstm_encoder.parameters()) + list(attn_decoder.parameters())\n",
    "optimizer = optim.Adam(params, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "I2XTxuom8jCO",
    "outputId": "d157b371-f85d-420d-8d37-6deb97be5864"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processes until data number 2000. cumulative loss for 2000 items is: 196.71745239257814\n",
      "processes until data number 4000. cumulative loss for 2000 items is: 149.11324182128905\n",
      "processes until data number 6000. cumulative loss for 2000 items is: 136.63633837890626\n",
      "processes until data number 8000. cumulative loss for 2000 items is: 132.33811865234375\n",
      "processes until data number 10000. cumulative loss for 2000 items is: 125.1440336303711\n",
      "processes until data number 12000. cumulative loss for 2000 items is: 124.79572412109376\n",
      "processes until data number 14000. cumulative loss for 2000 items is: 118.67991687011718\n",
      "processes until data number 16000. cumulative loss for 2000 items is: 115.84001318359375\n",
      "processes until data number 18000. cumulative loss for 2000 items is: 114.63165368652344\n",
      "processes until data number 20000. cumulative loss for 2000 items is: 112.67596661376953\n",
      "processes until data number 22000. cumulative loss for 2000 items is: 110.97641668701172\n",
      "processes until data number 24000. cumulative loss for 2000 items is: 110.79658001708984\n",
      "processes until data number 26000. cumulative loss for 2000 items is: 106.91930773925782\n",
      "processes until data number 28000. cumulative loss for 2000 items is: 104.30129516601562\n",
      "processes until data number 30000. cumulative loss for 2000 items is: 104.08499853515625\n",
      "processes until data number 32000. cumulative loss for 2000 items is: 98.56887078857422\n",
      "processes until data number 34000. cumulative loss for 2000 items is: 96.90726837158203\n",
      "processes until data number 36000. cumulative loss for 2000 items is: 92.92424285888671\n",
      "processes until data number 38000. cumulative loss for 2000 items is: 88.04499407958984\n",
      "processes until data number 40000. cumulative loss for 2000 items is: 84.26494549560547\n",
      "processes until data number 42000. cumulative loss for 2000 items is: 79.8629828491211\n",
      "processes until data number 44000. cumulative loss for 2000 items is: 75.5998447265625\n",
      "processes until data number 46000. cumulative loss for 2000 items is: 70.27279037475586\n",
      "processes until data number 48000. cumulative loss for 2000 items is: 68.74017086791993\n",
      "processes until data number 50000. cumulative loss for 2000 items is: 66.44280337524414\n",
      "processes until data number 52000. cumulative loss for 2000 items is: 60.343234741210935\n",
      "processes until data number 54000. cumulative loss for 2000 items is: 59.44112973022461\n",
      "processes until data number 56000. cumulative loss for 2000 items is: 55.818982055664065\n",
      "processes until data number 58000. cumulative loss for 2000 items is: 52.97299755859375\n",
      "processes until data number 60000. cumulative loss for 2000 items is: 49.86409536743164\n",
      "processes until data number 62000. cumulative loss for 2000 items is: 49.207077239990234\n",
      "processes until data number 64000. cumulative loss for 2000 items is: 45.364439056396485\n",
      "processes until data number 66000. cumulative loss for 2000 items is: 44.63852368164063\n",
      "----------------------------\n",
      "epoch 1 cumulative loss = 93.30858833283048\n",
      "----------------------------\n",
      "processes until data number 2000. cumulative loss for 2000 items is: 42.84309869384766\n",
      "processes until data number 4000. cumulative loss for 2000 items is: 42.43141273498535\n",
      "processes until data number 6000. cumulative loss for 2000 items is: 38.06514553833008\n",
      "processes until data number 8000. cumulative loss for 2000 items is: 39.489921676635745\n",
      "processes until data number 10000. cumulative loss for 2000 items is: 36.57293392944336\n",
      "processes until data number 12000. cumulative loss for 2000 items is: 36.14851594543457\n",
      "processes until data number 14000. cumulative loss for 2000 items is: 34.298051528930664\n",
      "processes until data number 16000. cumulative loss for 2000 items is: 34.29180319213867\n",
      "processes until data number 18000. cumulative loss for 2000 items is: 35.67100877380371\n",
      "processes until data number 20000. cumulative loss for 2000 items is: 32.68863818359375\n",
      "processes until data number 22000. cumulative loss for 2000 items is: 34.44390916442871\n",
      "processes until data number 24000. cumulative loss for 2000 items is: 32.84916622924805\n",
      "processes until data number 26000. cumulative loss for 2000 items is: 32.18820677185059\n",
      "processes until data number 28000. cumulative loss for 2000 items is: 30.323243423461914\n",
      "processes until data number 30000. cumulative loss for 2000 items is: 29.508468719482423\n",
      "processes until data number 32000. cumulative loss for 2000 items is: 28.477566955566406\n",
      "processes until data number 34000. cumulative loss for 2000 items is: 28.697881225585938\n",
      "processes until data number 36000. cumulative loss for 2000 items is: 27.709003967285156\n",
      "processes until data number 38000. cumulative loss for 2000 items is: 26.66911196899414\n",
      "processes until data number 40000. cumulative loss for 2000 items is: 28.815318466186522\n",
      "processes until data number 42000. cumulative loss for 2000 items is: 27.874369049072264\n",
      "processes until data number 44000. cumulative loss for 2000 items is: 26.755489318847655\n",
      "processes until data number 46000. cumulative loss for 2000 items is: 26.088585723876953\n",
      "processes until data number 48000. cumulative loss for 2000 items is: 26.778472290039062\n",
      "processes until data number 50000. cumulative loss for 2000 items is: 25.848667556762695\n",
      "processes until data number 52000. cumulative loss for 2000 items is: 32.142575805664066\n",
      "processes until data number 54000. cumulative loss for 2000 items is: 29.233213973999025\n",
      "processes until data number 56000. cumulative loss for 2000 items is: 25.40781756591797\n",
      "processes until data number 58000. cumulative loss for 2000 items is: 23.705202514648438\n",
      "processes until data number 60000. cumulative loss for 2000 items is: 23.890548202514648\n",
      "processes until data number 62000. cumulative loss for 2000 items is: 24.8482899017334\n",
      "processes until data number 64000. cumulative loss for 2000 items is: 23.443990310668944\n",
      "processes until data number 66000. cumulative loss for 2000 items is: 22.45282890319824\n",
      "----------------------------\n",
      "epoch 2 cumulative loss = 30.391449524310296\n",
      "----------------------------\n",
      "processes until data number 2000. cumulative loss for 2000 items is: 22.191178466796874\n",
      "processes until data number 4000. cumulative loss for 2000 items is: 22.775145561218263\n",
      "processes until data number 6000. cumulative loss for 2000 items is: 21.897826179504396\n",
      "processes until data number 8000. cumulative loss for 2000 items is: 42.87776390838623\n",
      "processes until data number 10000. cumulative loss for 2000 items is: 28.942695510864258\n",
      "processes until data number 12000. cumulative loss for 2000 items is: 25.793060272216795\n",
      "processes until data number 14000. cumulative loss for 2000 items is: 24.002481033325196\n",
      "processes until data number 16000. cumulative loss for 2000 items is: 22.65079965209961\n",
      "processes until data number 18000. cumulative loss for 2000 items is: 22.476005859375\n",
      "processes until data number 20000. cumulative loss for 2000 items is: 23.125139015197753\n",
      "processes until data number 22000. cumulative loss for 2000 items is: 21.268905555725098\n",
      "processes until data number 24000. cumulative loss for 2000 items is: 23.45779531097412\n",
      "processes until data number 26000. cumulative loss for 2000 items is: 21.353394218444823\n",
      "processes until data number 28000. cumulative loss for 2000 items is: 21.00385437774658\n",
      "processes until data number 30000. cumulative loss for 2000 items is: 24.428724769592286\n",
      "processes until data number 32000. cumulative loss for 2000 items is: 20.212335769653322\n",
      "processes until data number 34000. cumulative loss for 2000 items is: 20.53724757385254\n",
      "processes until data number 36000. cumulative loss for 2000 items is: 20.113452598571776\n",
      "processes until data number 38000. cumulative loss for 2000 items is: 18.15209287261963\n",
      "processes until data number 40000. cumulative loss for 2000 items is: 19.06940171813965\n",
      "processes until data number 42000. cumulative loss for 2000 items is: 25.856144302368165\n",
      "processes until data number 44000. cumulative loss for 2000 items is: 19.837124282836914\n",
      "processes until data number 46000. cumulative loss for 2000 items is: 18.242134284973144\n",
      "processes until data number 48000. cumulative loss for 2000 items is: 18.164365242004394\n",
      "processes until data number 50000. cumulative loss for 2000 items is: 19.078856491088867\n",
      "processes until data number 52000. cumulative loss for 2000 items is: 18.10174486541748\n",
      "processes until data number 54000. cumulative loss for 2000 items is: 17.653559356689453\n",
      "processes until data number 56000. cumulative loss for 2000 items is: 17.14935245513916\n",
      "processes until data number 58000. cumulative loss for 2000 items is: 17.820224617004396\n",
      "processes until data number 60000. cumulative loss for 2000 items is: 18.436816329956056\n",
      "processes until data number 62000. cumulative loss for 2000 items is: 18.718117317199706\n",
      "processes until data number 64000. cumulative loss for 2000 items is: 17.356079719543455\n",
      "processes until data number 66000. cumulative loss for 2000 items is: 16.814644302368166\n",
      "----------------------------\n",
      "epoch 3 cumulative loss = 21.33721643058514\n",
      "----------------------------\n",
      "processes until data number 2000. cumulative loss for 2000 items is: 17.280610855102537\n",
      "processes until data number 4000. cumulative loss for 2000 items is: 16.79503628540039\n",
      "processes until data number 6000. cumulative loss for 2000 items is: 15.936445594787598\n",
      "processes until data number 8000. cumulative loss for 2000 items is: 18.441549827575685\n",
      "processes until data number 10000. cumulative loss for 2000 items is: 16.668358657836915\n",
      "processes until data number 12000. cumulative loss for 2000 items is: 18.017463752746583\n",
      "processes until data number 14000. cumulative loss for 2000 items is: 25.006510131835938\n",
      "processes until data number 16000. cumulative loss for 2000 items is: 21.31218617248535\n",
      "processes until data number 18000. cumulative loss for 2000 items is: 18.376213844299315\n",
      "processes until data number 20000. cumulative loss for 2000 items is: 17.363693824768067\n",
      "processes until data number 22000. cumulative loss for 2000 items is: 21.310739715576172\n",
      "processes until data number 24000. cumulative loss for 2000 items is: 20.62591157531738\n",
      "processes until data number 26000. cumulative loss for 2000 items is: 18.381489265441896\n",
      "processes until data number 28000. cumulative loss for 2000 items is: 16.9869150390625\n",
      "processes until data number 30000. cumulative loss for 2000 items is: 17.14412831878662\n",
      "processes until data number 32000. cumulative loss for 2000 items is: 16.362845245361328\n",
      "processes until data number 34000. cumulative loss for 2000 items is: 17.753382232666016\n",
      "processes until data number 36000. cumulative loss for 2000 items is: 16.173571601867675\n",
      "processes until data number 38000. cumulative loss for 2000 items is: 16.335662574768065\n",
      "processes until data number 40000. cumulative loss for 2000 items is: 16.424048942565918\n",
      "processes until data number 42000. cumulative loss for 2000 items is: 15.074279754638672\n",
      "processes until data number 44000. cumulative loss for 2000 items is: 25.49756701660156\n",
      "processes until data number 46000. cumulative loss for 2000 items is: 16.91874584197998\n",
      "processes until data number 48000. cumulative loss for 2000 items is: 16.229084007263182\n",
      "processes until data number 50000. cumulative loss for 2000 items is: 20.616477897644042\n",
      "processes until data number 52000. cumulative loss for 2000 items is: 16.91727523803711\n",
      "processes until data number 54000. cumulative loss for 2000 items is: 15.645803367614747\n",
      "processes until data number 56000. cumulative loss for 2000 items is: 15.404698028564454\n",
      "processes until data number 58000. cumulative loss for 2000 items is: 14.66749754333496\n",
      "processes until data number 60000. cumulative loss for 2000 items is: 15.405805931091308\n",
      "processes until data number 62000. cumulative loss for 2000 items is: 16.351157272338867\n",
      "processes until data number 64000. cumulative loss for 2000 items is: 14.663491081237792\n",
      "processes until data number 66000. cumulative loss for 2000 items is: 14.455556785583497\n",
      "----------------------------\n",
      "epoch 4 cumulative loss = 17.45761335230366\n",
      "----------------------------\n",
      "processes until data number 2000. cumulative loss for 2000 items is: 14.478806640625\n",
      "processes until data number 4000. cumulative loss for 2000 items is: 14.48178875732422\n",
      "processes until data number 6000. cumulative loss for 2000 items is: 15.830324817657472\n",
      "processes until data number 8000. cumulative loss for 2000 items is: 19.038261627197265\n",
      "processes until data number 10000. cumulative loss for 2000 items is: 16.40297315979004\n",
      "processes until data number 12000. cumulative loss for 2000 items is: 23.41095696258545\n",
      "processes until data number 14000. cumulative loss for 2000 items is: 19.31023277282715\n",
      "processes until data number 16000. cumulative loss for 2000 items is: 17.209007461547852\n",
      "processes until data number 18000. cumulative loss for 2000 items is: 16.21633479309082\n",
      "processes until data number 20000. cumulative loss for 2000 items is: 15.852213912963867\n",
      "processes until data number 22000. cumulative loss for 2000 items is: 14.802573097229004\n",
      "processes until data number 24000. cumulative loss for 2000 items is: 14.679901412963867\n",
      "processes until data number 26000. cumulative loss for 2000 items is: 13.737987907409668\n",
      "processes until data number 28000. cumulative loss for 2000 items is: 13.745244415283203\n",
      "processes until data number 30000. cumulative loss for 2000 items is: 14.220199653625489\n",
      "processes until data number 32000. cumulative loss for 2000 items is: 18.15061476135254\n",
      "processes until data number 34000. cumulative loss for 2000 items is: 15.45500070953369\n",
      "processes until data number 36000. cumulative loss for 2000 items is: 13.862215026855468\n",
      "processes until data number 38000. cumulative loss for 2000 items is: 13.41532689666748\n",
      "processes until data number 40000. cumulative loss for 2000 items is: 14.345740005493164\n",
      "processes until data number 42000. cumulative loss for 2000 items is: 21.969826194763183\n",
      "processes until data number 44000. cumulative loss for 2000 items is: 18.726180953979494\n",
      "processes until data number 46000. cumulative loss for 2000 items is: 14.71986922454834\n",
      "processes until data number 48000. cumulative loss for 2000 items is: 14.448958824157716\n",
      "processes until data number 50000. cumulative loss for 2000 items is: 17.601673767089842\n",
      "processes until data number 52000. cumulative loss for 2000 items is: 15.272462303161621\n",
      "processes until data number 54000. cumulative loss for 2000 items is: 14.90792733001709\n",
      "processes until data number 56000. cumulative loss for 2000 items is: 13.949446075439454\n",
      "processes until data number 58000. cumulative loss for 2000 items is: 12.670083183288574\n",
      "processes until data number 60000. cumulative loss for 2000 items is: 13.190914016723633\n",
      "processes until data number 62000. cumulative loss for 2000 items is: 14.503452060699463\n",
      "processes until data number 64000. cumulative loss for 2000 items is: 19.302693885803222\n",
      "processes until data number 66000. cumulative loss for 2000 items is: 20.233889839172363\n",
      "----------------------------\n",
      "epoch 5 cumulative loss = 15.941995292392509\n",
      "----------------------------\n",
      "processes until data number 2000. cumulative loss for 2000 items is: 15.477164184570313\n",
      "processes until data number 4000. cumulative loss for 2000 items is: 18.434096282958983\n",
      "processes until data number 6000. cumulative loss for 2000 items is: 16.78418199920654\n",
      "processes until data number 8000. cumulative loss for 2000 items is: 16.132042358398436\n",
      "processes until data number 10000. cumulative loss for 2000 items is: 14.098925491333008\n",
      "processes until data number 12000. cumulative loss for 2000 items is: 15.606311180114746\n",
      "processes until data number 14000. cumulative loss for 2000 items is: 13.844936431884765\n",
      "processes until data number 16000. cumulative loss for 2000 items is: 13.640021766662597\n",
      "processes until data number 18000. cumulative loss for 2000 items is: 13.620931190490722\n",
      "processes until data number 20000. cumulative loss for 2000 items is: 14.483338562011719\n",
      "processes until data number 22000. cumulative loss for 2000 items is: 13.183665687561035\n",
      "processes until data number 24000. cumulative loss for 2000 items is: 15.654791076660157\n",
      "processes until data number 26000. cumulative loss for 2000 items is: 14.714752990722657\n",
      "processes until data number 28000. cumulative loss for 2000 items is: 13.65060227203369\n",
      "processes until data number 30000. cumulative loss for 2000 items is: 13.400425685882569\n",
      "processes until data number 32000. cumulative loss for 2000 items is: 12.803248119354247\n",
      "processes until data number 34000. cumulative loss for 2000 items is: 12.71781103515625\n",
      "processes until data number 36000. cumulative loss for 2000 items is: 12.227714859008788\n",
      "processes until data number 38000. cumulative loss for 2000 items is: 11.894648220062257\n",
      "processes until data number 40000. cumulative loss for 2000 items is: 11.913153266906738\n",
      "processes until data number 42000. cumulative loss for 2000 items is: 12.042718315124512\n",
      "processes until data number 44000. cumulative loss for 2000 items is: 12.508316543579102\n",
      "processes until data number 46000. cumulative loss for 2000 items is: 11.635060947418213\n",
      "processes until data number 48000. cumulative loss for 2000 items is: 13.287196952819825\n",
      "processes until data number 50000. cumulative loss for 2000 items is: 12.813777675628662\n",
      "processes until data number 52000. cumulative loss for 2000 items is: 12.366628204345703\n",
      "processes until data number 54000. cumulative loss for 2000 items is: 11.659241428375244\n",
      "processes until data number 56000. cumulative loss for 2000 items is: 11.33119554901123\n",
      "processes until data number 58000. cumulative loss for 2000 items is: 11.02222540664673\n",
      "processes until data number 60000. cumulative loss for 2000 items is: 11.445173461914063\n",
      "processes until data number 62000. cumulative loss for 2000 items is: 12.090107303619385\n",
      "processes until data number 64000. cumulative loss for 2000 items is: 11.612741428375244\n",
      "processes until data number 66000. cumulative loss for 2000 items is: 12.89293180847168\n",
      "----------------------------\n",
      "epoch 6 cumulative loss = 13.261064748720017\n",
      "----------------------------\n",
      "processes until data number 2000. cumulative loss for 2000 items is: 11.931625282287598\n",
      "processes until data number 4000. cumulative loss for 2000 items is: 12.027994865417481\n",
      "processes until data number 6000. cumulative loss for 2000 items is: 11.928560508728028\n",
      "processes until data number 8000. cumulative loss for 2000 items is: 14.155400024414062\n",
      "processes until data number 10000. cumulative loss for 2000 items is: 17.137990745544435\n",
      "processes until data number 12000. cumulative loss for 2000 items is: 19.880582962036133\n",
      "processes until data number 14000. cumulative loss for 2000 items is: 16.601980949401856\n",
      "processes until data number 16000. cumulative loss for 2000 items is: 13.823547943115233\n",
      "processes until data number 18000. cumulative loss for 2000 items is: 13.369867805480958\n",
      "processes until data number 20000. cumulative loss for 2000 items is: 13.378215705871582\n",
      "processes until data number 22000. cumulative loss for 2000 items is: 11.914221786499024\n",
      "processes until data number 24000. cumulative loss for 2000 items is: 12.982492477416992\n",
      "processes until data number 26000. cumulative loss for 2000 items is: 12.193712036132812\n",
      "processes until data number 28000. cumulative loss for 2000 items is: 13.36753677368164\n",
      "processes until data number 30000. cumulative loss for 2000 items is: 14.36445320892334\n",
      "processes until data number 32000. cumulative loss for 2000 items is: 13.149843578338624\n",
      "processes until data number 34000. cumulative loss for 2000 items is: 16.590020385742186\n",
      "processes until data number 36000. cumulative loss for 2000 items is: 18.79818544769287\n",
      "processes until data number 38000. cumulative loss for 2000 items is: 12.815171394348145\n",
      "processes until data number 40000. cumulative loss for 2000 items is: 12.694083572387695\n",
      "processes until data number 42000. cumulative loss for 2000 items is: 11.82255071258545\n",
      "processes until data number 44000. cumulative loss for 2000 items is: 13.023304237365723\n",
      "processes until data number 46000. cumulative loss for 2000 items is: 12.204330909729004\n",
      "processes until data number 48000. cumulative loss for 2000 items is: 12.418221153259278\n",
      "processes until data number 50000. cumulative loss for 2000 items is: 12.617773677825928\n",
      "processes until data number 52000. cumulative loss for 2000 items is: 11.517281734466552\n",
      "processes until data number 54000. cumulative loss for 2000 items is: 10.885476818084717\n",
      "processes until data number 56000. cumulative loss for 2000 items is: 10.364169811248779\n",
      "processes until data number 58000. cumulative loss for 2000 items is: 10.533822204589844\n",
      "processes until data number 60000. cumulative loss for 2000 items is: 11.065667247772216\n",
      "processes until data number 62000. cumulative loss for 2000 items is: 11.451157009124756\n",
      "processes until data number 64000. cumulative loss for 2000 items is: 10.460101718902587\n",
      "processes until data number 66000. cumulative loss for 2000 items is: 11.318618011474609\n",
      "----------------------------\n",
      "epoch 7 cumulative loss = 13.014417979518266\n",
      "----------------------------\n",
      "processes until data number 2000. cumulative loss for 2000 items is: 10.732151233673095\n",
      "processes until data number 4000. cumulative loss for 2000 items is: 10.910985363006592\n",
      "processes until data number 6000. cumulative loss for 2000 items is: 13.74388787460327\n",
      "processes until data number 8000. cumulative loss for 2000 items is: 17.312685623168946\n",
      "processes until data number 10000. cumulative loss for 2000 items is: 21.56475947570801\n",
      "processes until data number 12000. cumulative loss for 2000 items is: 18.213753658294678\n",
      "processes until data number 14000. cumulative loss for 2000 items is: 18.070612312316893\n",
      "processes until data number 16000. cumulative loss for 2000 items is: 13.703574054718018\n",
      "processes until data number 18000. cumulative loss for 2000 items is: 15.880036827087402\n",
      "processes until data number 20000. cumulative loss for 2000 items is: 13.395632698059082\n",
      "processes until data number 22000. cumulative loss for 2000 items is: 12.20355604171753\n",
      "processes until data number 24000. cumulative loss for 2000 items is: 12.68932839202881\n",
      "processes until data number 26000. cumulative loss for 2000 items is: 14.1435283203125\n",
      "processes until data number 28000. cumulative loss for 2000 items is: 13.271747997283935\n",
      "processes until data number 30000. cumulative loss for 2000 items is: 15.333444778442383\n",
      "processes until data number 32000. cumulative loss for 2000 items is: 13.70886874771118\n",
      "processes until data number 34000. cumulative loss for 2000 items is: 12.750665145874024\n",
      "processes until data number 36000. cumulative loss for 2000 items is: 12.123262664794922\n",
      "processes until data number 38000. cumulative loss for 2000 items is: 11.139567024230956\n",
      "processes until data number 40000. cumulative loss for 2000 items is: 13.380083763122558\n",
      "processes until data number 42000. cumulative loss for 2000 items is: 12.303438400268554\n",
      "processes until data number 44000. cumulative loss for 2000 items is: 13.591909496307373\n",
      "processes until data number 46000. cumulative loss for 2000 items is: 11.459489406585693\n",
      "processes until data number 48000. cumulative loss for 2000 items is: 10.824364418029786\n",
      "processes until data number 50000. cumulative loss for 2000 items is: 11.145708221435546\n",
      "processes until data number 52000. cumulative loss for 2000 items is: 11.069978610992433\n",
      "processes until data number 54000. cumulative loss for 2000 items is: 10.829934017181397\n",
      "processes until data number 56000. cumulative loss for 2000 items is: 10.185414070129395\n",
      "processes until data number 58000. cumulative loss for 2000 items is: 12.825520652770996\n",
      "processes until data number 60000. cumulative loss for 2000 items is: 14.412030097961425\n",
      "processes until data number 62000. cumulative loss for 2000 items is: 12.904791809082031\n",
      "processes until data number 64000. cumulative loss for 2000 items is: 11.464686351776123\n",
      "processes until data number 66000. cumulative loss for 2000 items is: 16.73865557861328\n",
      "----------------------------\n",
      "epoch 8 cumulative loss = 13.352420067277777\n",
      "----------------------------\n",
      "processes until data number 2000. cumulative loss for 2000 items is: 11.541309280395508\n",
      "processes until data number 4000. cumulative loss for 2000 items is: 13.85719896697998\n",
      "processes until data number 6000. cumulative loss for 2000 items is: 12.445986373901366\n",
      "processes until data number 8000. cumulative loss for 2000 items is: 13.384377250671387\n",
      "processes until data number 10000. cumulative loss for 2000 items is: 11.525177848815918\n",
      "processes until data number 12000. cumulative loss for 2000 items is: 11.55752995300293\n",
      "processes until data number 14000. cumulative loss for 2000 items is: 11.29603526687622\n",
      "processes until data number 16000. cumulative loss for 2000 items is: 11.162622081756592\n",
      "processes until data number 18000. cumulative loss for 2000 items is: 12.082276859283446\n",
      "processes until data number 20000. cumulative loss for 2000 items is: 12.272545181274413\n",
      "processes until data number 22000. cumulative loss for 2000 items is: 10.842534549713134\n",
      "processes until data number 24000. cumulative loss for 2000 items is: 11.55485661315918\n",
      "processes until data number 26000. cumulative loss for 2000 items is: 13.734786334991455\n",
      "processes until data number 28000. cumulative loss for 2000 items is: 12.922190898895265\n",
      "processes until data number 30000. cumulative loss for 2000 items is: 12.769623062133789\n",
      "processes until data number 32000. cumulative loss for 2000 items is: 11.587993099212646\n",
      "processes until data number 34000. cumulative loss for 2000 items is: 11.883012195587158\n",
      "processes until data number 36000. cumulative loss for 2000 items is: 12.330772846221924\n",
      "processes until data number 38000. cumulative loss for 2000 items is: 10.33055540084839\n",
      "processes until data number 40000. cumulative loss for 2000 items is: 11.125518089294433\n",
      "processes until data number 42000. cumulative loss for 2000 items is: 10.257855339050293\n",
      "processes until data number 44000. cumulative loss for 2000 items is: 10.224921474456787\n",
      "processes until data number 46000. cumulative loss for 2000 items is: 9.895822929382325\n",
      "processes until data number 48000. cumulative loss for 2000 items is: 10.801064468383789\n",
      "processes until data number 50000. cumulative loss for 2000 items is: 16.292244804382324\n",
      "processes until data number 52000. cumulative loss for 2000 items is: 24.100539833068847\n",
      "processes until data number 54000. cumulative loss for 2000 items is: 14.058877159118653\n",
      "processes until data number 56000. cumulative loss for 2000 items is: 11.835111099243164\n",
      "processes until data number 58000. cumulative loss for 2000 items is: 12.073135494232178\n",
      "processes until data number 60000. cumulative loss for 2000 items is: 16.921656044006347\n",
      "processes until data number 62000. cumulative loss for 2000 items is: 22.51360085296631\n",
      "processes until data number 64000. cumulative loss for 2000 items is: 33.71524203491211\n",
      "processes until data number 66000. cumulative loss for 2000 items is: 19.381241165161132\n",
      "----------------------------\n",
      "epoch 9 cumulative loss = 13.600511655606892\n",
      "----------------------------\n",
      "processes until data number 2000. cumulative loss for 2000 items is: 15.823975830078124\n",
      "processes until data number 4000. cumulative loss for 2000 items is: 16.56452778625488\n",
      "processes until data number 6000. cumulative loss for 2000 items is: 15.569949310302734\n",
      "processes until data number 8000. cumulative loss for 2000 items is: 17.093887077331544\n",
      "processes until data number 10000. cumulative loss for 2000 items is: 13.058325721740722\n",
      "processes until data number 12000. cumulative loss for 2000 items is: 13.236131721496582\n",
      "processes until data number 14000. cumulative loss for 2000 items is: 12.302145835876464\n",
      "processes until data number 16000. cumulative loss for 2000 items is: 11.95534638595581\n",
      "processes until data number 18000. cumulative loss for 2000 items is: 11.486788200378419\n",
      "processes until data number 20000. cumulative loss for 2000 items is: 11.803094646453857\n",
      "processes until data number 22000. cumulative loss for 2000 items is: 11.070993282318115\n",
      "processes until data number 24000. cumulative loss for 2000 items is: 11.992606273651123\n",
      "processes until data number 26000. cumulative loss for 2000 items is: 10.648532722473144\n",
      "processes until data number 28000. cumulative loss for 2000 items is: 13.345266124725342\n",
      "processes until data number 30000. cumulative loss for 2000 items is: 20.620894737243653\n",
      "processes until data number 32000. cumulative loss for 2000 items is: 14.09758977508545\n",
      "processes until data number 34000. cumulative loss for 2000 items is: 12.381351974487306\n",
      "processes until data number 36000. cumulative loss for 2000 items is: 11.717799388885497\n",
      "processes until data number 38000. cumulative loss for 2000 items is: 11.265053932189941\n",
      "processes until data number 40000. cumulative loss for 2000 items is: 11.357535972595215\n",
      "processes until data number 42000. cumulative loss for 2000 items is: 10.370609298706055\n",
      "processes until data number 44000. cumulative loss for 2000 items is: 11.446647930145264\n",
      "processes until data number 46000. cumulative loss for 2000 items is: 11.09947180557251\n",
      "processes until data number 48000. cumulative loss for 2000 items is: 10.74328196334839\n",
      "processes until data number 50000. cumulative loss for 2000 items is: 11.173406143188476\n",
      "processes until data number 52000. cumulative loss for 2000 items is: 12.787706260681153\n",
      "processes until data number 54000. cumulative loss for 2000 items is: 11.955450904846192\n",
      "processes until data number 56000. cumulative loss for 2000 items is: 11.593256484985352\n",
      "processes until data number 58000. cumulative loss for 2000 items is: 10.640100303649902\n",
      "processes until data number 60000. cumulative loss for 2000 items is: 11.152133903503419\n",
      "processes until data number 62000. cumulative loss for 2000 items is: 15.995171211242676\n",
      "processes until data number 64000. cumulative loss for 2000 items is: 13.441056434631347\n",
      "processes until data number 66000. cumulative loss for 2000 items is: 12.707576538085938\n",
      "----------------------------\n",
      "epoch 10 cumulative loss = 12.70497724765402\n",
      "----------------------------\n",
      "processes until data number 2000. cumulative loss for 2000 items is: 11.322081893920899\n",
      "processes until data number 4000. cumulative loss for 2000 items is: 11.733322189331055\n",
      "processes until data number 6000. cumulative loss for 2000 items is: 16.0010887298584\n",
      "processes until data number 8000. cumulative loss for 2000 items is: 13.538760330200196\n",
      "processes until data number 10000. cumulative loss for 2000 items is: 15.806800659179688\n",
      "processes until data number 12000. cumulative loss for 2000 items is: 13.177422149658202\n",
      "processes until data number 14000. cumulative loss for 2000 items is: 14.725431701660156\n",
      "processes until data number 16000. cumulative loss for 2000 items is: 12.47089421081543\n",
      "processes until data number 18000. cumulative loss for 2000 items is: 11.135831241607667\n",
      "processes until data number 20000. cumulative loss for 2000 items is: 11.282000144958497\n",
      "processes until data number 22000. cumulative loss for 2000 items is: 13.715881492614747\n",
      "processes until data number 24000. cumulative loss for 2000 items is: 12.963546165466308\n",
      "processes until data number 26000. cumulative loss for 2000 items is: 12.932661350250244\n",
      "processes until data number 28000. cumulative loss for 2000 items is: 25.637038871765135\n",
      "processes until data number 30000. cumulative loss for 2000 items is: 15.548632751464844\n",
      "processes until data number 32000. cumulative loss for 2000 items is: 13.669603630065918\n",
      "processes until data number 34000. cumulative loss for 2000 items is: 12.502839183807373\n",
      "processes until data number 36000. cumulative loss for 2000 items is: 11.337813705444336\n",
      "processes until data number 38000. cumulative loss for 2000 items is: 10.677593742370606\n",
      "processes until data number 40000. cumulative loss for 2000 items is: 11.289003986358642\n",
      "processes until data number 42000. cumulative loss for 2000 items is: 10.23784468460083\n",
      "processes until data number 44000. cumulative loss for 2000 items is: 10.388497165679931\n",
      "processes until data number 46000. cumulative loss for 2000 items is: 11.119111801147461\n",
      "processes until data number 48000. cumulative loss for 2000 items is: 10.99572571182251\n",
      "processes until data number 50000. cumulative loss for 2000 items is: 12.613769752502442\n",
      "processes until data number 52000. cumulative loss for 2000 items is: 13.286887237548829\n",
      "processes until data number 54000. cumulative loss for 2000 items is: 12.30146138381958\n",
      "processes until data number 56000. cumulative loss for 2000 items is: 10.582315406799317\n",
      "processes until data number 58000. cumulative loss for 2000 items is: 11.404516933441162\n",
      "processes until data number 60000. cumulative loss for 2000 items is: 12.350510070800782\n",
      "processes until data number 62000. cumulative loss for 2000 items is: 11.856871410369873\n",
      "processes until data number 64000. cumulative loss for 2000 items is: 10.788778800964355\n",
      "processes until data number 66000. cumulative loss for 2000 items is: 10.512846885681153\n",
      "----------------------------\n",
      "epoch 11 cumulative loss = 12.627084616397076\n",
      "----------------------------\n",
      "processes until data number 2000. cumulative loss for 2000 items is: 9.893596908569336\n",
      "processes until data number 4000. cumulative loss for 2000 items is: 10.293708934783936\n",
      "processes until data number 6000. cumulative loss for 2000 items is: 17.01771124267578\n",
      "processes until data number 8000. cumulative loss for 2000 items is: 15.292359275817871\n",
      "processes until data number 10000. cumulative loss for 2000 items is: 13.79970595550537\n",
      "processes until data number 12000. cumulative loss for 2000 items is: 12.441259365081788\n",
      "processes until data number 14000. cumulative loss for 2000 items is: 22.378936042785643\n",
      "processes until data number 16000. cumulative loss for 2000 items is: 14.52325447845459\n",
      "processes until data number 18000. cumulative loss for 2000 items is: 13.317031703948974\n",
      "processes until data number 20000. cumulative loss for 2000 items is: 12.810920219421387\n",
      "processes until data number 22000. cumulative loss for 2000 items is: 11.84643223953247\n",
      "processes until data number 24000. cumulative loss for 2000 items is: 12.227243816375733\n",
      "processes until data number 26000. cumulative loss for 2000 items is: 10.981975521087646\n",
      "processes until data number 28000. cumulative loss for 2000 items is: 11.667795650482178\n",
      "processes until data number 30000. cumulative loss for 2000 items is: 11.349675010681151\n",
      "processes until data number 32000. cumulative loss for 2000 items is: 11.238829685211181\n",
      "processes until data number 34000. cumulative loss for 2000 items is: 10.808699779510498\n",
      "processes until data number 36000. cumulative loss for 2000 items is: 12.82300227355957\n",
      "processes until data number 38000. cumulative loss for 2000 items is: 11.662913429260254\n",
      "processes until data number 40000. cumulative loss for 2000 items is: 11.553932518005372\n",
      "processes until data number 42000. cumulative loss for 2000 items is: 10.151004295349122\n",
      "processes until data number 44000. cumulative loss for 2000 items is: 15.682473609924317\n",
      "processes until data number 46000. cumulative loss for 2000 items is: 12.218654541015624\n",
      "processes until data number 48000. cumulative loss for 2000 items is: 11.698379093170166\n",
      "processes until data number 50000. cumulative loss for 2000 items is: 13.006565132141112\n",
      "processes until data number 52000. cumulative loss for 2000 items is: 11.581195915222168\n",
      "processes until data number 54000. cumulative loss for 2000 items is: 10.495136009216308\n",
      "processes until data number 56000. cumulative loss for 2000 items is: 9.699367931365966\n",
      "processes until data number 58000. cumulative loss for 2000 items is: 10.615402454376222\n",
      "processes until data number 60000. cumulative loss for 2000 items is: 11.15385534286499\n",
      "processes until data number 62000. cumulative loss for 2000 items is: 11.921545936584472\n",
      "processes until data number 64000. cumulative loss for 2000 items is: 10.257588207244874\n",
      "processes until data number 66000. cumulative loss for 2000 items is: 10.17011249923706\n",
      "----------------------------\n",
      "epoch 12 cumulative loss = 12.226323204933562\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(N_EPOCH):\n",
    "  batch_maker = make_batches()\n",
    "  epoch_loss, cum_loss = 0, 0\n",
    "  for j in range(int(n_train/BATCH_SIZE)+1):\n",
    "    x, y, max_len = next(batch_maker)\n",
    "    \n",
    "    cnn_encoded = cnn_encoder(x)\n",
    "    v_tilda, _ = lstm_encoder(cnn_encoded)\n",
    "    loss = attn_decoder.train(v_tilda, y, max_len, teacher_forcing_ratio=teacher_forcing)\n",
    "    cum_loss += loss.item()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if j % 100 == 0 and j != 0:\n",
    "      print(\"processes until data number {}. cumulative loss for {} items is: {}\".format(j*BATCH_SIZE, 100*BATCH_SIZE, cum_loss/(100*BATCH_SIZE)))\n",
    "      epoch_loss += cum_loss\n",
    "      cum_loss = 0\n",
    "  print(\"----------------------------\")\n",
    "  print(\"epoch {} cumulative loss = {}\".format(i+1, epoch_loss/n_train))\n",
    "  print(\"----------------------------\")\n",
    "  teacher_forcing -= 0.1\n",
    "  torch.save(cnn_encoder.state_dict(), '/content/drive/My Drive/Colab Notebooks/DL_Project/Saved_Models/cnn_train_0_epoch{}.pth'.format(i))\n",
    "  torch.save(lstm_encoder.state_dict(), '/content/drive/My Drive/Colab Notebooks/DL_Project/Saved_Models/lstm_train_0_epoch{}.pth'.format(i))\n",
    "  torch.save(attn_decoder.state_dict(), '/content/drive/My Drive/Colab Notebooks/DL_Project/Saved_Models/decoder_train_0_epoch{}.pth'.format(i))\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hpkZnoPC1n9d"
   },
   "outputs": [],
   "source": [
    "torch.save(cnn_encoder.state_dict(), '/content/drive/My Drive/Colab Notebooks/DL_Project/Saved_Models/new/cnn.pth')\n",
    "# torch.save(lstm_encoder.state_dict(), '/content/drive/My Drive/Colab Notebooks/DL_Project/Saved_Models/lstm.pth')\n",
    "# torch.save(attn_decoder.state_dict(), '/content/drive/My Drive/Colab Notebooks/DL_Project/Saved_Models/decoder.pth')\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "r-YGiJF6D3DV",
    "outputId": "116cf7d1-e540-4b12-df72-74ca32b3cc07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttnDecoder(\n",
       "  (embedding): Embedding(566, 80)\n",
       "  (lstm): LSTM(680, 512, batch_first=True)\n",
       "  (Wh): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (tanh): Tanh()\n",
       "  (beta): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (Wc1): Linear(in_features=512, out_features=600, bias=True)\n",
       "  (Wc2): Linear(in_features=512, out_features=600, bias=True)\n",
       "  (o_tanh): Tanh()\n",
       "  (Wout): Linear(in_features=600, out_features=566, bias=True)\n",
       "  (nll): NLLLoss()\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_encoder_loaded = nn.Sequential(\n",
    "  nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "  nn.ReLU(),\n",
    "  \n",
    "  nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0)),\n",
    "  \n",
    "  nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "  nn.ReLU(),\n",
    "  \n",
    "  nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0)),\n",
    "  \n",
    "  nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "  nn.BatchNorm2d(256),\n",
    "  nn.ReLU(),\n",
    "  \n",
    "  nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "  nn.ReLU(),\n",
    "  \n",
    "  nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=(0, 0)),\n",
    "  \n",
    "  nn.Conv2d(in_channels=256, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "  nn.BatchNorm2d(512),\n",
    "  nn.ReLU(),\n",
    "  \n",
    "  nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=(0, 0)),\n",
    "  \n",
    "  nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "  nn.BatchNorm2d(512),\n",
    "  nn.ReLU(),\n",
    "  \n",
    "  MyReshape()\n",
    "    \n",
    ")\n",
    "cnn_encoder_loaded.load_state_dict(torch.load('/content/drive/My Drive/Colab Notebooks/DL_Project/Saved_Models/cnn_train_0_epoch6.pth'))\n",
    "cnn_encoder_loaded.cuda()\n",
    "cnn_encoder_loaded.eval()\n",
    "\n",
    "lstm_encoder_loaded = nn.LSTM(feature_num, encoder_hidden_size, num_layers, bidirectional=True, batch_first=True)\n",
    "lstm_encoder_loaded.load_state_dict(torch.load('/content/drive/My Drive/Colab Notebooks/DL_Project/Saved_Models/lstm_train_0_epoch6.pth'))\n",
    "lstm_encoder_loaded.cuda()\n",
    "lstm_encoder_loaded.eval()\n",
    "\n",
    "attn_decoder_loaded = AttnDecoder(num_tokens, embedding_dim=80, hidden_dim=512, output_dim=600, attn_dim=512)\n",
    "attn_decoder_loaded.load_state_dict(torch.load('/content/drive/My Drive/Colab Notebooks/DL_Project/Saved_Models/decoder_train_0_epoch6.pth'))\n",
    "attn_decoder_loaded.cuda()\n",
    "# attn_decoder_loaded.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FVfA22VZJD4e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mfWgeRHB0r3S",
    "outputId": "27d6786f-a38c-4acd-9735-f8830926dfbf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1049105"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 0\n",
    "validation_preds = \"\"\n",
    "for index in range(n_test):\n",
    "  test_image = torch.Tensor(images_test[index]).cuda()\n",
    "  test_image -= 128\n",
    "  test_image /= 128\n",
    "  formula = predict(test_image, cnn_encoder_loaded, lstm_encoder_loaded, attn_decoder_loaded)\n",
    "  for i in range(len(formula)):\n",
    "    formula[i] = all_tokens[formula[i]]\n",
    "  # print(formula)\n",
    "  sentence = \"\"\n",
    "  for i in range(1, len(formula)-1):\n",
    "    sentence += formula[i]\n",
    "    sentence += \" \"\n",
    "  validation_preds += sentence\n",
    "  validation_preds += \"\\n\"\n",
    "val_preds = open('test_formulas_prediction.txt', 'w')\n",
    "val_preds.write(validation_preds)\n",
    "# print(sentence)\n",
    "# formula = ''.join(formula[1:-1], )\n",
    "# print(formula)\n",
    "\n",
    "# plt.imshow(images_validation[index].reshape(60,400))\n",
    "# plt.show()\n",
    "# print(images_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "colab_type": "code",
    "id": "pb-jg656jx5O",
    "outputId": "792a243e-609a-492e-e39c-a354392e479a"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-89e95dcfc7e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpylatex\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSubsection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTikZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPlot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPackage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpylatex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpylatex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitalic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mescape_latex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pylatex.numpy'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pylatex import Document, Section, Subsection, Table, Math, TikZ, Axis, Plot, Figure, Package\n",
    "from pylatex.numpy import Matrix\n",
    "from pylatex.utils import italic, escape_latex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qxGxffkSkhnU"
   },
   "outputs": [],
   "source": [
    "import pylatex."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DL_Project.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
